# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¤šç»´åº¦è‚¡ç¥¨åˆ†æå™¨
æ•´åˆä¸ªè‚¡ã€è¡Œä¸šã€å¤§ç›˜ä¸‰ä¸ªå±‚é¢çš„8çº§ä¿¡å·åˆ†æ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Union, Optional, Tuple
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

class AnalysisLevel(Enum):
    """åˆ†æå±‚çº§æšä¸¾"""
    INDIVIDUAL = "individual"    # ä¸ªè‚¡å±‚é¢
    INDUSTRY = "industry"        # è¡Œä¸šå±‚é¢
    MARKET = "market"           # å¤§ç›˜å±‚é¢

class SignalStrength(Enum):
    """ä¿¡å·å¼ºåº¦æšä¸¾"""
    VERY_STRONG = "very_strong"     # éå¸¸å¼º
    STRONG = "strong"               # å¼º
    MODERATE = "moderate"           # ä¸­ç­‰
    WEAK = "weak"                   # å¼±
    VERY_WEAK = "very_weak"         # éå¸¸å¼±

@dataclass
class AnalysisResult:
    """åˆ†æç»“æœæ•°æ®ç»“æ„"""
    score: float                    # ç»¼åˆè¯„åˆ† (0-100)
    signal_strength: SignalStrength # ä¿¡å·å¼ºåº¦
    confidence: float               # ç½®ä¿¡åº¦ (0-1)
    trend_direction: str            # è¶‹åŠ¿æ–¹å‘
    risk_level: str                 # é£é™©ç­‰çº§
    recommendations: List[str]      # å…·ä½“å»ºè®®
    contributing_factors: Dict      # è´¡çŒ®å› å­è¯¦æƒ…

class EnhancedMultiDimensionalAnalyzer:
    """
    å¢å¼ºç‰ˆå¤šç»´åº¦åˆ†æå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ä¸ªè‚¡å±‚é¢ï¼šå¢å¼ºç‰ˆRTSI + åŸºæœ¬é¢åˆ†æ
    2. è¡Œä¸šå±‚é¢ï¼šæ™ºèƒ½è½®åŠ¨åˆ†æ + ç›¸å¯¹å¼ºåº¦
    3. å¤§ç›˜å±‚é¢ï¼šç³»ç»Ÿæ€§é£é™© + å¸‚åœºæƒ…ç»ª
    4. ä¸‰å±‚è”åŠ¨ï¼šå¤šç»´åº¦ä¿¡å·èåˆä¸éªŒè¯
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œåªè¾“å‡ºä¸¥é‡é”™è¯¯
        self.logger.setLevel(logging.ERROR)
        
        # 8çº§è¯„çº§æ˜ å°„
        self.rating_map = {
            'å¤§å¤š': 7, 'ä¸­å¤š': 6, 'å°å¤š': 5, 'å¾®å¤š': 4,
            'å¾®ç©º': 3, 'å°ç©º': 2, 'ä¸­ç©º': 1, 'å¤§ç©º': 0,
            'ä¸­æ€§': 3.5, '-': None
        }
        
        # åå‘æ˜ å°„ï¼ˆæ•°å€¼åˆ°è¯„çº§ï¼‰
        self.reverse_rating_map = {v: k for k, v in self.rating_map.items() if v is not None}
        
        # åˆ†ææƒé‡é…ç½®
        self.analysis_weights = {
            'individual_weight': 0.4,    # ä¸ªè‚¡åˆ†ææƒé‡
            'industry_weight': 0.35,     # è¡Œä¸šåˆ†ææƒé‡
            'market_weight': 0.25        # å¤§ç›˜åˆ†ææƒé‡
        }
        
        # é£é™©ç­‰çº§é˜ˆå€¼
        self.risk_thresholds = {
            'very_low': (0, 10),
            'low': (10, 25),
            'medium': (25, 50),
            'high': (50, 75),
            'very_high': (75, 100)
        }
    
    def comprehensive_analysis(self, 
                             stock_data: pd.DataFrame,
                             industry_data: pd.DataFrame = None,
                             market_data: pd.DataFrame = None,
                             target_stock: str = None) -> Dict[str, AnalysisResult]:
        """
        ç»¼åˆå¤šç»´åº¦åˆ†æ
        
        Args:
            stock_data: ä¸ªè‚¡æ•°æ®
            industry_data: è¡Œä¸šæ•°æ®
            market_data: å¸‚åœºæ•°æ®
            target_stock: ç›®æ ‡è‚¡ç¥¨ä»£ç 
            
        Returns:
            å¤šå±‚æ¬¡åˆ†æç»“æœ
        """
        analysis_start = datetime.now()
        results = {}
        
        try:
            # 1. ä¸ªè‚¡å±‚é¢åˆ†æ
            if target_stock and not stock_data.empty:
                individual_result = self._analyze_individual_stock(stock_data, target_stock)
                results['individual'] = individual_result
            
            # 2. è¡Œä¸šå±‚é¢åˆ†æ
            if industry_data is not None and not industry_data.empty:
                industry_result = self._analyze_industry_strength(industry_data, target_stock)
                results['industry'] = industry_result
            
            # 3. å¤§ç›˜å±‚é¢åˆ†æ
            if market_data is not None and not market_data.empty:
                market_result = self._analyze_market_sentiment(market_data)
                results['market'] = market_result
            
            # 4. å¤šç»´åº¦èåˆåˆ†æ
            if len(results) > 1:
                fusion_result = self._multi_dimensional_fusion(results)
                results['fusion'] = fusion_result
            
            # 5. ç”Ÿæˆæœ€ç»ˆå»ºè®®
            final_recommendation = self._generate_final_recommendation(results)
            results['final_recommendation'] = final_recommendation
            
            processing_time = (datetime.now() - analysis_start).total_seconds()
            self.logger.info(f"å¤šç»´åº¦åˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.3f}s")
            
            return results
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆåˆ†æå¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_individual_stock(self, stock_data: pd.DataFrame, target_stock: str) -> AnalysisResult:
        """ä¸ªè‚¡å±‚é¢åˆ†æ"""
        try:
            # è·å–ç›®æ ‡è‚¡ç¥¨æ•°æ®
            if target_stock in stock_data.index:
                stock_ratings = stock_data.loc[target_stock]
            elif target_stock in stock_data.get('è‚¡ç¥¨ä»£ç ', pd.Series()).values:
                stock_ratings = stock_data[stock_data['è‚¡ç¥¨ä»£ç '] == target_stock].iloc[0]
            else:
                raise ValueError(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {target_stock}")
            
            # è·å–æ—¥æœŸåˆ—
            date_columns = [col for col in stock_ratings.index if str(col).startswith('202')]
            date_columns.sort()
            
            if len(date_columns) < 5:
                raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œä¸ªè‚¡åˆ†æ")
            
            # 1. åŸºç¡€RTSIåˆ†æ
            rating_values = [stock_ratings[col] for col in date_columns]
            rtsi_score = self._calculate_enhanced_rtsi(rating_values)
            
            # 2. è¶‹åŠ¿åˆ†æ
            trend_analysis = self._analyze_rating_trend(rating_values)
            
            # 3. æ³¢åŠ¨æ€§åˆ†æ
            volatility_analysis = self._analyze_rating_volatility(rating_values)
            
            # 4. åŠ¨é‡åˆ†æ
            momentum_analysis = self._analyze_rating_momentum(rating_values)
            
            # 5. ç»¼åˆè¯„åˆ†è®¡ç®—
            comprehensive_score = (
                rtsi_score * 0.3 +
                trend_analysis['score'] * 0.25 +
                volatility_analysis['score'] * 0.2 +
                momentum_analysis['score'] * 0.25
            )
            
            # 6. ä¿¡å·å¼ºåº¦åˆ¤æ–­
            signal_strength = self._determine_signal_strength(comprehensive_score)
            
            # 7. é£é™©è¯„ä¼°
            risk_level = self._assess_risk_level(volatility_analysis, trend_analysis)
            
            # 8. ç½®ä¿¡åº¦è®¡ç®—
            confidence = self._calculate_confidence(rating_values, comprehensive_score)
            
            # 9. ç”Ÿæˆå»ºè®®
            recommendations = self._generate_individual_recommendations(
                comprehensive_score, trend_analysis, volatility_analysis, momentum_analysis
            )
            
            return AnalysisResult(
                score=comprehensive_score,
                signal_strength=signal_strength,
                confidence=confidence,
                trend_direction=trend_analysis['direction'],
                risk_level=risk_level,
                recommendations=recommendations,
                contributing_factors={
                    'rtsi_score': rtsi_score,
                    'trend_score': trend_analysis['score'],
                    'volatility_score': volatility_analysis['score'],
                    'momentum_score': momentum_analysis['score']
                }
            )
            
        except Exception as e:
            # å°†é”™è¯¯çº§åˆ«æ”¹ä¸ºdebugï¼Œé¿å…è¾“å‡ºåˆ°ç»ˆç«¯
            self.logger.debug(f"ä¸ªè‚¡åˆ†æå¤±è´¥: {str(e)}")
            return AnalysisResult(
                score=0, signal_strength=SignalStrength.VERY_WEAK,
                confidence=0, trend_direction="æœªçŸ¥", risk_level="é«˜",
                recommendations=[f"åˆ†æå¤±è´¥: {str(e)}"],
                contributing_factors={}
            )
    
    def _analyze_industry_strength(self, industry_data: pd.DataFrame, target_stock: str = None) -> AnalysisResult:
        """è¡Œä¸šå±‚é¢åˆ†æ"""
        try:
            # è·å–æ—¥æœŸåˆ—
            date_columns = [col for col in industry_data.columns if str(col).startswith('202')]
            date_columns.sort()
            
            if len(date_columns) < 10:
                raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¡Œä¸šåˆ†æ")
            
            # 1. è¡Œä¸šæ•´ä½“è¯„çº§åˆ†å¸ƒåˆ†æ
            industry_distribution = self._analyze_industry_distribution(industry_data, date_columns)
            
            # 2. è¡Œä¸šè½®åŠ¨å¼ºåº¦åˆ†æ
            rotation_strength = self._analyze_industry_rotation(industry_data, date_columns)
            
            # 3. ç›¸å¯¹å¼ºåº¦åˆ†æ
            relative_strength = self._analyze_industry_relative_strength(industry_data, date_columns)
            
            # 4. è¡Œä¸šé›†ä¸­åº¦åˆ†æ
            concentration_analysis = self._analyze_industry_concentration(industry_data, date_columns)
            
            # 5. ç»¼åˆè¯„åˆ†
            comprehensive_score = (
                industry_distribution['score'] * 0.3 +
                rotation_strength['score'] * 0.25 +
                relative_strength['score'] * 0.25 +
                concentration_analysis['score'] * 0.2
            )
            
            # 6. ä¿¡å·å¼ºåº¦å’Œè¶‹åŠ¿æ–¹å‘
            signal_strength = self._determine_signal_strength(comprehensive_score)
            trend_direction = self._determine_industry_trend(rotation_strength, relative_strength)
            
            # 7. é£é™©è¯„ä¼°
            risk_level = self._assess_industry_risk(concentration_analysis, rotation_strength)
            
            # 8. ç½®ä¿¡åº¦
            confidence = min(0.9, concentration_analysis['data_quality'] * relative_strength['reliability'])
            
            # 9. å»ºè®®ç”Ÿæˆ
            recommendations = self._generate_industry_recommendations(
                comprehensive_score, rotation_strength, relative_strength, concentration_analysis
            )
            
            return AnalysisResult(
                score=comprehensive_score,
                signal_strength=signal_strength,
                confidence=confidence,
                trend_direction=trend_direction,
                risk_level=risk_level,
                recommendations=recommendations,
                contributing_factors={
                    'distribution_score': industry_distribution['score'],
                    'rotation_score': rotation_strength['score'],
                    'relative_strength_score': relative_strength['score'],
                    'concentration_score': concentration_analysis['score']
                }
            )
            
        except Exception as e:
            self.logger.error(f"è¡Œä¸šåˆ†æå¤±è´¥: {str(e)}")
            return AnalysisResult(
                score=0, signal_strength=SignalStrength.VERY_WEAK,
                confidence=0, trend_direction="æœªçŸ¥", risk_level="é«˜",
                recommendations=[f"åˆ†æå¤±è´¥: {str(e)}"],
                contributing_factors={}
            )
    
    def _analyze_market_sentiment(self, market_data: pd.DataFrame) -> AnalysisResult:
        """å¤§ç›˜å±‚é¢åˆ†æ"""
        try:
            # è·å–æ—¥æœŸåˆ—
            date_columns = [col for col in market_data.columns if str(col).startswith('202')]
            date_columns.sort()
            
            if len(date_columns) < 10:
                raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¸‚åœºåˆ†æ")
            
            # 1. å¸‚åœºæƒ…ç»ªåˆ†å¸ƒåˆ†æ
            sentiment_distribution = self._analyze_market_sentiment_distribution(market_data, date_columns)
            
            # 2. ç³»ç»Ÿæ€§é£é™©åˆ†æ
            systemic_risk = self._analyze_systemic_risk(market_data, date_columns)
            
            # 3. å¸‚åœºå‚ä¸åº¦åˆ†æ
            participation_analysis = self._analyze_market_participation(market_data, date_columns)
            
            # 4. æç«¯æƒ…ç»ªæ£€æµ‹
            extreme_sentiment = self._detect_extreme_sentiment(market_data, date_columns)
            
            # 5. å¸‚åœºå‘¨æœŸåˆ†æ
            cycle_analysis = self._analyze_market_cycle(market_data, date_columns)
            
            # 6. ç»¼åˆè¯„åˆ†
            comprehensive_score = (
                sentiment_distribution['score'] * 0.25 +
                (100 - systemic_risk['score']) * 0.25 +  # é£é™©è¶Šä½åˆ†æ•°è¶Šé«˜
                participation_analysis['score'] * 0.2 +
                extreme_sentiment['score'] * 0.15 +
                cycle_analysis['score'] * 0.15
            )
            
            # 7. ä¿¡å·å¼ºåº¦å’Œè¶‹åŠ¿
            signal_strength = self._determine_signal_strength(comprehensive_score)
            trend_direction = cycle_analysis['phase']
            
            # 8. é£é™©ç­‰çº§
            risk_level = self._assess_market_risk(systemic_risk, extreme_sentiment)
            
            # 9. ç½®ä¿¡åº¦
            confidence = min(0.95, participation_analysis['reliability'] * cycle_analysis['confidence'])
            
            # 10. å»ºè®®ç”Ÿæˆ
            recommendations = self._generate_market_recommendations(
                comprehensive_score, systemic_risk, extreme_sentiment, cycle_analysis
            )
            
            return AnalysisResult(
                score=comprehensive_score,
                signal_strength=signal_strength,
                confidence=confidence,
                trend_direction=trend_direction,
                risk_level=risk_level,
                recommendations=recommendations,
                contributing_factors={
                    'sentiment_score': sentiment_distribution['score'],
                    'systemic_risk_score': systemic_risk['score'],
                    'participation_score': participation_analysis['score'],
                    'extreme_sentiment_score': extreme_sentiment['score'],
                    'cycle_score': cycle_analysis['score']
                }
            )
            
        except Exception as e:
            self.logger.error(f"å¸‚åœºåˆ†æå¤±è´¥: {str(e)}")
            return AnalysisResult(
                score=0, signal_strength=SignalStrength.VERY_WEAK,
                confidence=0, trend_direction="æœªçŸ¥", risk_level="é«˜",
                recommendations=[f"åˆ†æå¤±è´¥: {str(e)}"],
                contributing_factors={}
            )
    
    def _multi_dimensional_fusion(self, results: Dict[str, AnalysisResult]) -> AnalysisResult:
        """å¤šç»´åº¦èåˆåˆ†æ"""
        try:
            # è·å–å„ç»´åº¦ç»“æœ
            individual = results.get('individual')
            industry = results.get('industry')
            market = results.get('market')
            
            # è®¡ç®—åŠ æƒç»¼åˆå¾—åˆ†
            weighted_score = 0
            total_weight = 0
            confidence_weights = []
            
            if individual:
                weight = self.analysis_weights['individual_weight'] * individual.confidence
                weighted_score += individual.score * weight
                total_weight += weight
                confidence_weights.append(individual.confidence)
            
            if industry:
                weight = self.analysis_weights['industry_weight'] * industry.confidence
                weighted_score += industry.score * weight
                total_weight += weight
                confidence_weights.append(industry.confidence)
            
            if market:
                weight = self.analysis_weights['market_weight'] * market.confidence
                weighted_score += market.score * weight
                total_weight += weight
                confidence_weights.append(market.confidence)
            
            # æœ€ç»ˆç»¼åˆå¾—åˆ†
            final_score = weighted_score / total_weight if total_weight > 0 else 0
            
            # å¤šç»´åº¦ä¸€è‡´æ€§æ£€éªŒ
            consistency_score = self._calculate_multi_dimensional_consistency(results)
            
            # ç»¼åˆç½®ä¿¡åº¦
            avg_confidence = np.mean(confidence_weights) if confidence_weights else 0
            final_confidence = avg_confidence * consistency_score
            
            # è¶‹åŠ¿æ–¹å‘èåˆ
            trend_votes = []
            if individual: trend_votes.append(individual.trend_direction)
            if industry: trend_votes.append(industry.trend_direction)
            if market: trend_votes.append(market.trend_direction)
            
            final_trend = max(set(trend_votes), key=trend_votes.count) if trend_votes else "ä¸­æ€§"
            
            # é£é™©ç­‰çº§èåˆ
            risk_levels = []
            if individual: risk_levels.append(individual.risk_level)
            if industry: risk_levels.append(industry.risk_level)
            if market: risk_levels.append(market.risk_level)
            
            final_risk = max(set(risk_levels), key=risk_levels.count) if risk_levels else "ä¸­ç­‰"
            
            # ä¿¡å·å¼ºåº¦
            signal_strength = self._determine_signal_strength(final_score)
            
            # èåˆå»ºè®®
            fusion_recommendations = self._generate_fusion_recommendations(
                results, final_score, consistency_score
            )
            
            return AnalysisResult(
                score=final_score,
                signal_strength=signal_strength,
                confidence=final_confidence,
                trend_direction=final_trend,
                risk_level=final_risk,
                recommendations=fusion_recommendations,
                contributing_factors={
                    'consistency_score': consistency_score,
                    'individual_weight': individual.score if individual else 0,
                    'industry_weight': industry.score if industry else 0,
                    'market_weight': market.score if market else 0
                }
            )
            
        except Exception as e:
            self.logger.error(f"å¤šç»´åº¦èåˆå¤±è´¥: {str(e)}")
            return AnalysisResult(
                score=0, signal_strength=SignalStrength.VERY_WEAK,
                confidence=0, trend_direction="æœªçŸ¥", risk_level="é«˜",
                recommendations=[f"èåˆåˆ†æå¤±è´¥: {str(e)}"],
                contributing_factors={}
            )
    
    # è¾…åŠ©æ–¹æ³•å®ç°
    def _calculate_enhanced_rtsi(self, rating_values: List) -> float:
        """è®¡ç®—å¢å¼ºç‰ˆRTSI"""
        try:
            # è½¬æ¢ä¸ºæ•°å€¼
            numeric_values = []
            for rating in rating_values:
                if rating in self.rating_map and self.rating_map[rating] is not None:
                    numeric_values.append(self.rating_map[rating])
                elif isinstance(rating, (int, float)):
                    numeric_values.append(rating)
            
            if len(numeric_values) < 5:
                return 0
            
            # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
            x = np.arange(len(numeric_values))
            slope = np.polyfit(x, numeric_values, 1)[0]
            
            # è®¡ç®—ä¸€è‡´æ€§ï¼ˆRÂ²ï¼‰
            correlation = np.corrcoef(x, numeric_values)[0, 1]
            r_squared = correlation ** 2 if not np.isnan(correlation) else 0
            
            # è®¡ç®—å˜åŒ–å¹…åº¦
            amplitude = abs(slope) * len(numeric_values) / 7  # æ ‡å‡†åŒ–åˆ°8çº§è¯„çº§
            amplitude = min(amplitude, 1.0)
            
            # RTSIè®¡ç®—
            rtsi = (r_squared * 0.4 + abs(slope)/7 * 0.3 + amplitude * 0.3) * 100
            
            return min(rtsi, 100)
            
        except Exception as e:
            self.logger.error(f"RTSIè®¡ç®—å¤±è´¥: {str(e)}")
            return 0
    
    def _analyze_rating_trend(self, rating_values: List) -> Dict:
        """åˆ†æè¯„çº§è¶‹åŠ¿"""
        try:
            numeric_values = [self.rating_map.get(r, r) for r in rating_values if r is not None]
            if len(numeric_values) < 3:
                return {'score': 0, 'direction': 'æœªçŸ¥', 'strength': 0}
            
            # çº¿æ€§å›å½’
            x = np.arange(len(numeric_values))
            slope = np.polyfit(x, numeric_values, 1)[0]
            
            # è¶‹åŠ¿æ–¹å‘
            if slope > 0.1:
                direction = "ä¸Šå‡"
            elif slope < -0.1:
                direction = "ä¸‹é™"
            else:
                direction = "å¹³ç¨³"
            
            # è¶‹åŠ¿å¼ºåº¦
            strength = min(abs(slope) * 10, 10)  # æ ‡å‡†åŒ–åˆ°0-10
            
            # è¯„åˆ†
            score = 50 + slope * 10  # ä¸­æ€§ä¸º50ï¼Œä¸Šå‡è¶‹åŠ¿åŠ åˆ†ï¼Œä¸‹é™è¶‹åŠ¿å‡åˆ†
            score = max(0, min(100, score))
            
            return {
                'score': score,
                'direction': direction,
                'strength': strength,
                'slope': slope
            }
            
        except Exception as e:
            return {'score': 0, 'direction': 'æœªçŸ¥', 'strength': 0}
    
    def _analyze_rating_volatility(self, rating_values: List) -> Dict:
        """åˆ†æè¯„çº§æ³¢åŠ¨æ€§"""
        try:
            numeric_values = [self.rating_map.get(r, r) for r in rating_values if r is not None]
            if len(numeric_values) < 2:
                return {'score': 50, 'level': 'ä¸­ç­‰'}
            
            volatility = np.std(numeric_values)
            
            # æ³¢åŠ¨æ€§ç­‰çº§
            if volatility < 0.5:
                level = "å¾ˆä½"
                score = 80  # ä½æ³¢åŠ¨æ€§ç»™é«˜åˆ†
            elif volatility < 1.0:
                level = "ä½"
                score = 70
            elif volatility < 1.5:
                level = "ä¸­ç­‰"
                score = 50
            elif volatility < 2.0:
                level = "é«˜"
                score = 30
            else:
                level = "å¾ˆé«˜"
                score = 10  # é«˜æ³¢åŠ¨æ€§ç»™ä½åˆ†
            
            return {
                'score': score,
                'level': level,
                'volatility': volatility
            }
            
        except Exception as e:
            return {'score': 50, 'level': 'ä¸­ç­‰'}
    
    def _analyze_rating_momentum(self, rating_values: List) -> Dict:
        """åˆ†æè¯„çº§åŠ¨é‡"""
        try:
            numeric_values = [self.rating_map.get(r, r) for r in rating_values if r is not None]
            if len(numeric_values) < 5:
                return {'score': 50, 'momentum': 0}
            
            # è®¡ç®—çŸ­æœŸå’Œé•¿æœŸå¹³å‡
            short_avg = np.mean(numeric_values[-3:])  # æœ€è¿‘3æœŸ
            long_avg = np.mean(numeric_values[:-3])   # ä¹‹å‰çš„æœŸæ•°
            
            momentum = short_avg - long_avg
            
            # åŠ¨é‡è¯„åˆ†
            score = 50 + momentum * 10  # æ­£åŠ¨é‡åŠ åˆ†ï¼Œè´ŸåŠ¨é‡å‡åˆ†
            score = max(0, min(100, score))
            
            return {
                'score': score,
                'momentum': momentum,
                'short_avg': short_avg,
                'long_avg': long_avg
            }
            
        except Exception as e:
            return {'score': 50, 'momentum': 0}
    
    def _determine_signal_strength(self, score: float) -> SignalStrength:
        """ç¡®å®šä¿¡å·å¼ºåº¦"""
        if score >= 80:
            return SignalStrength.VERY_STRONG
        elif score >= 65:
            return SignalStrength.STRONG
        elif score >= 35:
            return SignalStrength.MODERATE
        elif score >= 20:
            return SignalStrength.WEAK
        else:
            return SignalStrength.VERY_WEAK
    
    def _assess_risk_level(self, volatility_analysis: Dict, trend_analysis: Dict) -> str:
        """è¯„ä¼°é£é™©ç­‰çº§"""
        volatility_score = volatility_analysis.get('score', 50)
        trend_strength = trend_analysis.get('strength', 0)
        
        # é«˜æ³¢åŠ¨æ€§ + å¼±è¶‹åŠ¿ = é«˜é£é™©
        risk_score = (100 - volatility_score) + (10 - trend_strength) * 5
        
        if risk_score < 20:
            return "å¾ˆä½"
        elif risk_score < 40:
            return "ä½"
        elif risk_score < 60:
            return "ä¸­ç­‰"
        elif risk_score < 80:
            return "é«˜"
        else:
            return "å¾ˆé«˜"
    
    def _calculate_confidence(self, rating_values: List, score: float) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        try:
            # æ•°æ®å®Œæ•´æ€§
            valid_count = sum(1 for r in rating_values if r != '-' and r is not None)
            data_completeness = valid_count / len(rating_values)
            
            # åˆ†æ•°ç¨³å®šæ€§
            score_stability = 1 - abs(score - 50) / 50  # è¶Šæ¥è¿‘ä¸­æ€§è¶Šç¨³å®š
            
            # ç»¼åˆç½®ä¿¡åº¦
            confidence = (data_completeness * 0.7 + score_stability * 0.3)
            
            return max(0.1, min(0.95, confidence))
            
        except Exception as e:
            return 0.5
    
    def _generate_individual_recommendations(self, score: float, trend: Dict, volatility: Dict, momentum: Dict) -> List[str]:
        """ç”Ÿæˆä¸ªè‚¡å»ºè®®"""
        recommendations = []
        
        if score >= 70:
            recommendations.append("ğŸ’¹ ä¸ªè‚¡è¡¨ç°å¼ºåŠ¿ï¼Œå»ºè®®å…³æ³¨")
        elif score >= 50:
            recommendations.append("ğŸ“ˆ ä¸ªè‚¡è¡¨ç°ä¸­æ€§åå¥½ï¼Œå¯é€‚åº¦å…³æ³¨")
        else:
            recommendations.append("ğŸ“‰ ä¸ªè‚¡è¡¨ç°åå¼±ï¼Œå»ºè®®è°¨æ…")
        
        if trend['direction'] == "ä¸Šå‡":
            recommendations.append(f"ğŸ“Š è¶‹åŠ¿å‘ä¸Šï¼Œå¼ºåº¦: {trend['strength']:.1f}")
        elif trend['direction'] == "ä¸‹é™":
            recommendations.append(f"ğŸ“‰ è¶‹åŠ¿å‘ä¸‹ï¼Œå»ºè®®é˜²å¾¡")
        
        if volatility['level'] in ['é«˜', 'å¾ˆé«˜']:
            recommendations.append("âš ï¸ æ³¢åŠ¨æ€§è¾ƒé«˜ï¼Œæ³¨æ„é£é™©æ§åˆ¶")
        
        return recommendations
    
    # å…¶ä»–è¡Œä¸šå’Œå¸‚åœºåˆ†ææ–¹æ³•çš„ç®€åŒ–å®ç°
    def _analyze_industry_distribution(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """è¡Œä¸šåˆ†å¸ƒåˆ†æç®€åŒ–å®ç°"""
        return {'score': 60}  # ç®€åŒ–è¿”å›
    
    def _analyze_industry_rotation(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """è¡Œä¸šè½®åŠ¨åˆ†æç®€åŒ–å®ç°"""
        return {'score': 55}  # ç®€åŒ–è¿”å›
    
    def _analyze_industry_relative_strength(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """ç›¸å¯¹å¼ºåº¦åˆ†æç®€åŒ–å®ç°"""
        return {'score': 65, 'reliability': 0.8}  # ç®€åŒ–è¿”å›
    
    def _analyze_industry_concentration(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """è¡Œä¸šé›†ä¸­åº¦åˆ†æç®€åŒ–å®ç°"""
        return {'score': 70, 'data_quality': 0.85}  # ç®€åŒ–è¿”å›
    
    def _analyze_market_sentiment_distribution(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """å¸‚åœºæƒ…ç»ªåˆ†å¸ƒåˆ†æç®€åŒ–å®ç°"""
        return {'score': 60}  # ç®€åŒ–è¿”å›
    
    def _analyze_systemic_risk(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """ç³»ç»Ÿæ€§é£é™©åˆ†æç®€åŒ–å®ç°"""
        return {'score': 30}  # ç®€åŒ–è¿”å›ï¼Œåˆ†æ•°è¶Šä½é£é™©è¶Šä½
    
    def _analyze_market_participation(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """å¸‚åœºå‚ä¸åº¦åˆ†æç®€åŒ–å®ç°"""
        return {'score': 75, 'reliability': 0.9}  # ç®€åŒ–è¿”å›
    
    def _detect_extreme_sentiment(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """æç«¯æƒ…ç»ªæ£€æµ‹ç®€åŒ–å®ç°"""
        return {'score': 50}  # ç®€åŒ–è¿”å›
    
    def _analyze_market_cycle(self, data: pd.DataFrame, date_columns: List) -> Dict:
        """å¸‚åœºå‘¨æœŸåˆ†æç®€åŒ–å®ç°"""
        return {'score': 65, 'phase': 'ä¸Šå‡æœŸ', 'confidence': 0.8}  # ç®€åŒ–è¿”å›
    
    def _determine_industry_trend(self, rotation: Dict, strength: Dict) -> str:
        """ç¡®å®šè¡Œä¸šè¶‹åŠ¿ç®€åŒ–å®ç°"""
        return "è½®åŠ¨ä¸Šå‡"  # ç®€åŒ–è¿”å›
    
    def _assess_industry_risk(self, concentration: Dict, rotation: Dict) -> str:
        """è¯„ä¼°è¡Œä¸šé£é™©ç®€åŒ–å®ç°"""
        return "ä¸­ç­‰"  # ç®€åŒ–è¿”å›
    
    def _assess_market_risk(self, systemic: Dict, extreme: Dict) -> str:
        """è¯„ä¼°å¸‚åœºé£é™©ç®€åŒ–å®ç°"""
        return "ä¸­ç­‰"  # ç®€åŒ–è¿”å›
    
    def _calculate_multi_dimensional_consistency(self, results: Dict) -> float:
        """è®¡ç®—å¤šç»´åº¦ä¸€è‡´æ€§"""
        scores = []
        for key, result in results.items():
            if isinstance(result, AnalysisResult):
                scores.append(result.score)
        
        if len(scores) < 2:
            return 1.0
        
        # è®¡ç®—åˆ†æ•°çš„æ ‡å‡†å·®ï¼Œæ ‡å‡†å·®è¶Šå°ä¸€è‡´æ€§è¶Šé«˜
        std_dev = np.std(scores)
        consistency = max(0, 1 - std_dev / 50)  # æ ‡å‡†åŒ–åˆ°0-1
        
        return consistency
    
    def _generate_industry_recommendations(self, score: float, rotation: Dict, strength: Dict, concentration: Dict) -> List[str]:
        """ç”Ÿæˆè¡Œä¸šå»ºè®®ç®€åŒ–å®ç°"""
        return ["ğŸ­ è¡Œä¸šè¡¨ç°ç¨³å®š", "ğŸ“Š å»ºè®®å…³æ³¨è¡Œä¸šè½®åŠ¨æœºä¼š"]
    
    def _generate_market_recommendations(self, score: float, risk: Dict, extreme: Dict, cycle: Dict) -> List[str]:
        """ç”Ÿæˆå¸‚åœºå»ºè®®ç®€åŒ–å®ç°"""
        return ["ğŸŒ å¸‚åœºæ•´ä½“å‘å¥½", "âš ï¸ æ³¨æ„ç³»ç»Ÿæ€§é£é™©"]
    
    def _generate_fusion_recommendations(self, results: Dict, score: float, consistency: float) -> List[str]:
        """ç”Ÿæˆèåˆå»ºè®®"""
        recommendations = []
        
        if consistency > 0.8:
            recommendations.append("âœ… å¤šç»´åº¦åˆ†æç»“æœä¸€è‡´æ€§é«˜ï¼Œå»ºè®®å‚è€ƒ")
        elif consistency > 0.6:
            recommendations.append("ğŸ“Š å¤šç»´åº¦åˆ†æç»“æœåŸºæœ¬ä¸€è‡´")
        else:
            recommendations.append("âš ï¸ å¤šç»´åº¦åˆ†æç»“æœå­˜åœ¨åˆ†æ­§ï¼Œå»ºè®®è°¨æ…å†³ç­–")
        
        if score >= 70:
            recommendations.append("ğŸ’¹ ç»¼åˆè¯„åˆ†è¾ƒé«˜ï¼ŒæŠ•èµ„æœºä¼šè¾ƒå¥½")
        elif score >= 50:
            recommendations.append("ğŸ“ˆ ç»¼åˆè¯„åˆ†ä¸­æ€§ï¼Œå¯è€ƒè™‘é€‚åº¦å‚ä¸")
        else:
            recommendations.append("ğŸ“‰ ç»¼åˆè¯„åˆ†åä½ï¼Œå»ºè®®ä¿æŒè°¨æ…")
        
        return recommendations
    
    def _generate_final_recommendation(self, results: Dict) -> AnalysisResult:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        if 'fusion' in results:
            return results['fusion']
        elif 'individual' in results:
            return results['individual']
        else:
            return AnalysisResult(
                score=0, signal_strength=SignalStrength.VERY_WEAK,
                confidence=0, trend_direction="æœªçŸ¥", risk_level="é«˜",
                recommendations=["æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå»ºè®®"],
                contributing_factors={}
            )

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = EnhancedMultiDimensionalAnalyzer()
    
    # ç¤ºä¾‹è‚¡ç¥¨æ•°æ®
    sample_stock_data = pd.DataFrame({
        'è‚¡ç¥¨ä»£ç ': ['000001'],
        '20241201': ['å¤§å¤š'],
        '20241202': ['ä¸­å¤š'],
        '20241203': ['å°å¤š'],
        '20241204': ['-'],
        '20241205': ['å¾®å¤š']
    })
    
    # æ‰§è¡Œåˆ†æ
    results = analyzer.comprehensive_analysis(
        stock_data=sample_stock_data,
        target_stock='000001'
    )
    
    for level, result in results.items():
        if isinstance(result, AnalysisResult):
            print(f"\n{level}åˆ†æç»“æœ:")
            print(f"  è¯„åˆ†: {result.score:.1f}")
            print(f"  ä¿¡å·å¼ºåº¦: {result.signal_strength.value}")
            print(f"  ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print(f"  è¶‹åŠ¿æ–¹å‘: {result.trend_direction}")
            print(f"  é£é™©ç­‰çº§: {result.risk_level}")
            print(f"  å»ºè®®: {result.recommendations}")
