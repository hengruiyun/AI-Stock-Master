"""
from config.i18n import t_gui as _
Excelæ•°æ®åŠ è½½å™¨ - å¢å¼ºç‰ˆæ•°æ®åŠ è½½å’ŒéªŒè¯

åŠŸèƒ½ç‰¹æ€§ï¼š
1. å¤šç§æ•°æ®æ ¼å¼æ”¯æŒ (*.json.gz)
2. è‡ªåŠ¨æ–‡ä»¶æ—¥æœŸæ£€æµ‹
3. æ•°æ®ç»“æ„éªŒè¯
4. 8çº§è¯„çº§ç³»ç»Ÿæ”¯æŒ
5. æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼ˆCSVæ–‡ä»¶ï¼‰
6. é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

åŸºäºï¼šxlsx_analyzer.py åŠŸèƒ½æ‰©å±•
ä½œè€…: 267278466@qq.com
åˆ›å»ºæ—¶é—´ï¼š2025-06-07
"""

import pandas as pd
import numpy as np
import re
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime
import os
import logging
from pathlib import Path

# å¯¼å…¥é…ç½®
try:
    from config import RATING_SCORE_MAP, MARKET_CONFIG
except ImportError:
    # é»˜è®¤é…ç½®
    RATING_SCORE_MAP = {
        'å¤§å¤š': 7, 'ä¸­å¤š': 6, 'å°å¤š': 5, 'å¾®å¤š': 4,
        'å¾®ç©º': 3, 'å°ç©º': 2, 'ä¸­ç©º': 1, 'å¤§ç©º': 0, 
        '-': None
    }
    MARKET_CONFIG = {'CN': {'name': 'ä¸­å›½Aè‚¡'}}

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ExcelDataLoader:
    """å¢å¼ºçš„æ•°æ®åŠ è½½å™¨ - æ”¯æŒExcelå’ŒCSVæ ¼å¼"""
    
    def __init__(self, file_path: str):
        """
        åˆå§‹åŒ–åŠ è½½å™¨
        
        å‚æ•°:
            file_path (str): Excelæ–‡ä»¶è·¯å¾„
        """
        self.file_path = file_path
        self.data = None
        self.validation_result = {}
        self.file_info = {}
        self.load_time = None
        
    def load_and_validate(self) -> Tuple[Optional[pd.DataFrame], Dict]:
        """
        åŠ è½½å¹¶éªŒè¯æ•°æ®
        
        è¿”å›:
            tuple: (DataFrameå¯¹è±¡, éªŒè¯ç»“æœå­—å…¸)
        """
        load_start = datetime.now()
        
        try:
            logger.info(f"å¼€å§‹åŠ è½½Excelæ–‡ä»¶: {self.file_path}")
            
            # 1. æ–‡ä»¶æ£€æŸ¥
            file_check = self._check_file()
            if not file_check['is_valid']:
                return None, file_check
            
            # 2. åŠ è½½æ•°æ®
            self.data = self._load_excel_data()
            if self.data is None:
                return None, {'is_valid': False, 'error': 'Excelæ–‡ä»¶åŠ è½½å¤±è´¥'}
            
            # 3. æ•°æ®éªŒè¯
            self.validation_result = self._validate_data_structure()
            
            # 4. æ•°æ®æ¸…æ´—
            if self.validation_result['is_valid']:
                self.data = self._clean_data()
            
            # 5. è®°å½•åŠ è½½æ—¶é—´
            self.load_time = (datetime.now() - load_start).total_seconds()
            self.validation_result['load_time'] = f"{self.load_time:.2f}s"
            self.validation_result['file_info'] = self.file_info
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {self.data.shape if self.data is not None else 'Failed'}")
            
            return self.data, self.validation_result
            
        except Exception as e:
            error_msg = f"æ•°æ®åŠ è½½å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            return None, {'is_valid': False, 'error': error_msg}
    
    def detect_file_date(self) -> str:
        """
        ä»æ–‡ä»¶åè‡ªåŠ¨æ£€æµ‹æ•°æ®æ—¥æœŸ
        
        è¿”å›:
            str: æ£€æµ‹åˆ°çš„æ—¥æœŸ (YYYYMMDDæ ¼å¼)
        """
        filename = os.path.basename(self.file_path)
        
        # åŒ¹é…YYYYMMDDæ ¼å¼
        date_pattern = r'20\d{6}'
        match = re.search(date_pattern, filename)
        
        if match:
            return match.group()
        
        # å¦‚æœæ–‡ä»¶åä¸­æ²¡æœ‰æ—¥æœŸï¼Œå°è¯•ä»ä¿®æ”¹æ—¶é—´è·å–
        try:
            file_mtime = os.path.getmtime(self.file_path)
            file_date = datetime.fromtimestamp(file_mtime)
            return file_date.strftime('%Y%m%d')
        except:
            return datetime.now().strftime('%Y%m%d')
    
    def get_data_summary(self) -> Dict:
        """
        è·å–æ•°æ®æ¦‚è§ˆ
        
        è¿”å›:
            dict: æ•°æ®æ¦‚è§ˆä¿¡æ¯
        """
        if self.data is None:
            return {}
        
        date_columns = [col for col in self.data.columns if str(col).startswith('202')]
        
        # è¯„çº§åˆ†å¸ƒç»Ÿè®¡
        rating_stats = {}
        if date_columns:
            latest_col = max(date_columns)
            rating_dist = self.data[latest_col].value_counts()
            total_rated = sum(count for rating, count in rating_dist.items() if rating != '-')
            
            for rating, count in rating_dist.items():
                percentage = count / len(self.data) * 100
                rating_stats[rating] = {
                    'count': count,
                    'percentage': round(percentage, 1)
                }
        
        # è¡Œä¸šåˆ†å¸ƒ
        industry_stats = {}
        if 'è¡Œä¸š' in self.data.columns:
            industry_dist = self.data['è¡Œä¸š'].value_counts()
            for industry, count in industry_dist.head(10).items():
                percentage = count / len(self.data) * 100
                industry_stats[industry] = {
                    'count': count,
                    'percentage': round(percentage, 1)
                }
        
        return {
            'total_stocks': len(self.data),
            'total_columns': len(self.data.columns),
            'date_range': f"{min(date_columns)} ~ {max(date_columns)}" if date_columns else "æ— ",
            'trading_days': len(date_columns),
            'industry_coverage': self.data['è¡Œä¸š'].notna().sum() / len(self.data) * 100 if 'è¡Œä¸š' in self.data.columns else 0,
            'latest_rating_stats': rating_stats,
            'top_industries': industry_stats,
            'file_date': self.detect_file_date(),
            'load_time': self.load_time
        }
    
    # ç§æœ‰æ–¹æ³•
    
    def _check_file(self) -> Dict:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œå¯è¯»"""
        if not os.path.exists(self.file_path):
            return {'is_valid': False, 'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {self.file_path}'}
        
        if not os.path.isfile(self.file_path):
            return {'is_valid': False, 'error': f'ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {self.file_path}'}
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        _, ext = os.path.splitext(self.file_path)
        if ext.lower() not in ['.xlsx', '.xls', '.csv']:
            return {'is_valid': False, 'error': f'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}'}
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_stat = os.stat(self.file_path)
        self.file_info = {
            'file_name': os.path.basename(self.file_path),
            'file_size': file_stat.st_size,
            'file_size_mb': round(file_stat.st_size / 1024 / 1024, 2),
            'modified_time': datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
            'detected_date': self.detect_file_date()
        }
        
        return {'is_valid': True}
    
    def _load_excel_data(self) -> Optional[pd.DataFrame]:
        """åŠ è½½Excelæˆ–CSVæ•°æ®"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            _, ext = os.path.splitext(self.file_path)
            
            if ext.lower() == '.csv':
                # åŠ è½½CSVæ–‡ä»¶
                try:
                    # å°è¯•ä¸åŒçš„ç¼–ç 
                    encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(self.file_path, encoding=encoding)
                            logger.info(f"ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸåŠ è½½CSVæ•°æ®")
                            return df
                        except UnicodeDecodeError:
                            continue
                    
                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•é»˜è®¤æ–¹å¼
                    df = pd.read_csv(self.file_path)
                    logger.info("ä½¿ç”¨é»˜è®¤ç¼–ç æˆåŠŸåŠ è½½CSVæ•°æ®")
                    return df
                    
                except Exception as e:
                    logger.error(f"CSVåŠ è½½å¤±è´¥: {e}")
                    return None
            else:
                # åŠ è½½Excelæ–‡ä»¶
                # å°è¯•ä¸åŒçš„å¼•æ“
                engines = ['openpyxl', 'xlrd']
                
                for engine in engines:
                    try:
                        df = pd.read_excel(self.file_path, engine=engine)
                        logger.info(f"ä½¿ç”¨å¼•æ“ {engine} æˆåŠŸåŠ è½½Excelæ•°æ®")
                        return df
                    except Exception as e:
                        logger.warning(f"å¼•æ“ {engine} åŠ è½½å¤±è´¥: {e}")
                        continue
                
                # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±è´¥ï¼Œå°è¯•é»˜è®¤æ–¹å¼
                return pd.read_excel(self.file_path)
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _validate_data_structure(self) -> Dict:
        """éªŒè¯æ•°æ®ç»“æ„"""
        if self.data is None:
            return {'is_valid': False, 'error': 'æ•°æ®ä¸ºç©º'}
        
        validation = {
            'is_valid': True,
            'missing_columns': [],
            'warnings': [],
            'data_quality': {}
        }
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ['è¡Œä¸š', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°']
        for col in required_columns:
            if col not in self.data.columns:
                validation['missing_columns'].append(col)
                validation['is_valid'] = False
        
        # æ£€æŸ¥æ—¥æœŸåˆ—
        date_columns = [col for col in self.data.columns if str(col).startswith('202')]
        
        if len(date_columns) == 0:
            validation['warnings'].append('æœªæ‰¾åˆ°æ—¥æœŸåˆ—')
            validation['is_valid'] = False
        elif len(date_columns) < 5:
            validation['warnings'].append('æ—¥æœŸåˆ—è¿‡å°‘ï¼Œå¯èƒ½å½±å“è¶‹åŠ¿åˆ†æå‡†ç¡®æ€§')
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        if validation['is_valid']:
            validation['data_quality'] = {
                'total_rows': len(self.data),
                'total_columns': len(self.data.columns),
                'date_columns_count': len(date_columns),
                'date_range': f"{min(date_columns)} ~ {max(date_columns)}" if date_columns else "æ— ",
                'industry_coverage': self.data['è¡Œä¸š'].notna().sum() / len(self.data) * 100,
                'duplicate_codes': self.data['è‚¡ç¥¨ä»£ç '].duplicated().sum(),
                'duplicate_names': self.data['è‚¡ç¥¨åç§°'].duplicated().sum()
            }
            
            # æ£€æŸ¥è¯„çº§æœ‰æ•ˆæ€§
            rating_check = self._check_rating_validity(date_columns)
            validation['data_quality']['rating_validity'] = rating_check
        
        return validation
    
    def _check_rating_validity(self, date_columns: List[str]) -> Dict:
        """æ£€æŸ¥è¯„çº§æ•°æ®æœ‰æ•ˆæ€§"""
        valid_ratings = set(RATING_SCORE_MAP.keys())
        invalid_ratings = {}
        
        for col in date_columns[:5]:  # æ£€æŸ¥å‰5åˆ—
            unique_ratings = set(self.data[col].dropna().unique())
            invalid = unique_ratings - valid_ratings
            if invalid:
                invalid_ratings[col] = list(invalid)
        
        total_ratings = 0
        valid_rating_count = 0
        
        for col in date_columns:
            col_ratings = self.data[col].dropna()
            total_ratings += len(col_ratings)
            valid_rating_count += col_ratings.isin(valid_ratings).sum()
        
        validity_rate = valid_rating_count / total_ratings * 100 if total_ratings > 0 else 0
        
        return {
            'validity_rate': round(validity_rate, 2),
            'total_ratings': total_ratings,
            'valid_ratings': valid_rating_count,
            'invalid_ratings': invalid_ratings
        }
    
    def _detect_market_type(self) -> str:
        """æ£€æµ‹æ•°æ®æ–‡ä»¶çš„å¸‚åœºç±»å‹"""
        if not self.file_path:
            return 'cn'  # é»˜è®¤ä¸­å›½å¸‚åœº
        
        file_name = Path(self.file_path).name.lower()
        
        # æ ¹æ®æ–‡ä»¶åå‰ç¼€åˆ¤æ–­å¸‚åœºç±»å‹
        if file_name.startswith('us'):
            return 'us'
        elif file_name.startswith('hk'):
            return 'hk'
        elif file_name.startswith('cn'):
            return 'cn'
        else:
            # æ ¹æ®æ–‡ä»¶åå…³é”®è¯åˆ¤æ–­
            if 'us' in file_name or 'america' in file_name or 'usa' in file_name:
                return 'us'
            elif 'hk' in file_name or 'hongkong' in file_name or 'hong' in file_name:
                return 'hk'
            elif 'cn' in file_name or 'china' in file_name:
                return 'cn'
            else:
                # æ£€æŸ¥è‚¡ç¥¨ä»£ç æ ¼å¼æ¥æ¨æ–­å¸‚åœºç±»å‹
                if self.data is not None and 'è‚¡ç¥¨ä»£ç ' in self.data.columns:
                    sample_codes = self.data['è‚¡ç¥¨ä»£ç '].head(10).astype(str)
                    # å¦‚æœå¤§éƒ¨åˆ†ä»£ç åŒ…å«å­—æ¯ï¼Œå¯èƒ½æ˜¯USå¸‚åœº
                    alpha_count = sum(1 for code in sample_codes if any(c.isalpha() for c in str(code)))
                    if alpha_count > len(sample_codes) * 0.7:  # 70%ä»¥ä¸ŠåŒ…å«å­—æ¯
                        return 'us'
                
                logger.info(f"æ— æ³•ä»æ–‡ä»¶åè¯†åˆ«å¸‚åœºç±»å‹: {file_name}ï¼Œé»˜è®¤ä½¿ç”¨CNå¸‚åœº")
                return 'cn'
    
    def _clean_data(self) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        df = self.data.copy()
        
        # æ£€æµ‹å¸‚åœºç±»å‹
        market_type = self._detect_market_type()
        logger.info(f"æ£€æµ‹åˆ°å¸‚åœºç±»å‹: {market_type.upper()}")
        
        # 1. è‚¡ç¥¨ä»£ç æ ‡å‡†åŒ– - æ ¹æ®å¸‚åœºç±»å‹å¤„ç†
        if 'è‚¡ç¥¨ä»£ç ' in df.columns:
            if market_type == 'us':
                # USå¸‚åœºä¿æŒåŸå§‹ä»£ç æ ¼å¼ï¼ˆå­—æ¯ä»£ç å¦‚AAPL, MSFTï¼‰
                df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].astype(str).str.strip().str.upper()
                logger.info("USå¸‚åœº: ä¿æŒåŸå§‹è‚¡ç¥¨ä»£ç æ ¼å¼")
            else:
                # CN/HKå¸‚åœºä½¿ç”¨6ä½æ•°å­—å¡«å……
                df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6)
                logger.info(f"{market_type.upper()}å¸‚åœº: ä½¿ç”¨6ä½æ•°å­—å¡«å……æ ¼å¼")
        
        # 2. è¡Œä¸šä¿¡æ¯å¤„ç†
        if 'è¡Œä¸š' in df.columns:
            df['è¡Œä¸š'] = df['è¡Œä¸š'].fillna('æœªåˆ†ç±»')
            # å»é™¤ç©ºç™½å­—ç¬¦
            df['è¡Œä¸š'] = df['è¡Œä¸š'].astype(str).str.strip()
        
        # 3. è‚¡ç¥¨åç§°å¤„ç†
        if 'è‚¡ç¥¨åç§°' in df.columns:
            df['è‚¡ç¥¨åç§°'] = df['è‚¡ç¥¨åç§°'].astype(str).str.strip()
        
        # 4. è¯„çº§æ•°æ®å¤„ç† - ç¡®ä¿ç¬¦åˆ8çº§è¯„çº§ç³»ç»Ÿ
        date_columns = [col for col in df.columns if str(col).startswith('202')]
        for col in date_columns:
            # å°†æ— æ•ˆè¯„çº§æ›¿æ¢ä¸º'-'
            valid_ratings = set(RATING_SCORE_MAP.keys())
            mask = ~df[col].isin(valid_ratings)
            df.loc[mask, col] = '-'
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {len(df)} è¡Œæ•°æ®")
        return df


# ä¾¿æ·å‡½æ•°

def load_stock_data(file_path: str, auto_clean: bool = True) -> Tuple[Optional[pd.DataFrame], Dict]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŠ è½½è‚¡ç¥¨æ•°æ®
    
    å‚æ•°:
        file_path (str): Excelæ–‡ä»¶è·¯å¾„
        auto_clean (bool): æ˜¯å¦è‡ªåŠ¨æ¸…æ´—æ•°æ®
        
    è¿”å›:
        tuple: (DataFrame, åŠ è½½ç»“æœ)
    """
    loader = ExcelDataLoader(file_path)
    return loader.load_and_validate()


def detect_market_region(data: pd.DataFrame) -> str:
    """
    æ£€æµ‹å¸‚åœºåœ°åŒº
    
    å‚æ•°:
        data (pd.DataFrame): è‚¡ç¥¨æ•°æ®
        
    è¿”å›:
        str: å¸‚åœºåœ°åŒºä»£ç  (CN/HK/US)
    """
    if 'è‚¡ç¥¨ä»£ç ' not in data.columns:
        return 'CN'  # é»˜è®¤Aè‚¡
    
    codes = data['è‚¡ç¥¨ä»£ç '].astype(str).tolist()
    
    # æ£€æŸ¥æ¸¯è‚¡ç‰¹å¾
    hk_pattern = re.compile(r'^\d{5}$|\.HK$')
    hk_count = sum(1 for code in codes if hk_pattern.match(code))
    
    # æ£€æŸ¥ç¾è‚¡ç‰¹å¾
    us_pattern = re.compile(r'^[A-Z]{1,5}$')
    us_count = sum(1 for code in codes if us_pattern.match(code))
    
    # æ£€æŸ¥Aè‚¡ç‰¹å¾
    cn_pattern = re.compile(r'^\d{6}$')
    cn_count = sum(1 for code in codes if cn_pattern.match(code))
    
    # è¿”å›å æ¯”æœ€é«˜çš„å¸‚åœº
    total = len(codes)
    if cn_count / total > 0.7:
        return 'CN'
    elif hk_count / total > 0.7:
        return 'HK'
    elif us_count / total > 0.7:
        return 'US'
    else:
        return 'CN'  # é»˜è®¤Aè‚¡


# æµ‹è¯•å‡½æ•°
def test_excel_loader():
    """æµ‹è¯•ExcelåŠ è½½å™¨"""
    print("Excel...")
    
    test_file = "CN_Data.json.gz"
    if not os.path.exists(test_file):
        print(f"   è­¦å‘Š æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return False
    
    # æµ‹è¯•åŠ è½½
    loader = ExcelDataLoader(test_file)
    data, result = loader.load_and_validate()
    
    if result.get('is_valid', False):
        print(f"   æˆåŠŸ åŠ è½½æˆåŠŸ: {data.shape}")
        print(f"   æ•°æ® æ•°æ®æ¦‚è§ˆ: {loader.get_data_summary()}")
        
        # æµ‹è¯•å¸‚åœºæ£€æµ‹
        region = detect_market_region(data)
        print(f"   ğŸŒ æ£€æµ‹å¸‚åœº: {region}")
        
    else:
        print(f"   é”™è¯¯ åŠ è½½å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False
    
    print("Excel")
    return True


if __name__ == "__main__":
    test_excel_loader()